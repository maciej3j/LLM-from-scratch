{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90629667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import ipdb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import requests, zipfile, io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# thsese improve performance for Ampere architecture\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5187d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_url = \"https://ideami.com/llm_train\"\n",
    "# print(\"Downloading dataset...\")\n",
    "# response = requests.get(files_url)\n",
    "# zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039f7684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# architecture parameters\n",
    "batch_size = 8\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = True\n",
    "\n",
    "# hyperparameters\n",
    "lr = 3e-4\n",
    "dropout = 0.05\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "\n",
    "# training parameters\n",
    "train_iters = 100000\n",
    "eval_interval = 50\n",
    "eval_iters = 10\n",
    "compile = True\n",
    "checkpoint_dir = 'models/'\n",
    "checkpoint_fn = 'latest.pt'\n",
    "checkpoint_load_fn = 'latest.pt'\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# MODE \n",
    "inference = False\n",
    "\n",
    "# DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a39a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaciejej\u001b[0m (\u001b[33mmaciejej-uniwersytet-dzki\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/maciej/workspace/LLMUdemy/1/wandb/run-20251213_221137-0ae7ihsi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm1/runs/0ae7ihsi' target=\"_blank\">llm1-2025_12_13_22_11_36</a></strong> to <a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm1' target=\"_blank\">https://wandb.ai/maciejej-uniwersytet-dzki/llm1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm1/runs/0ae7ihsi' target=\"_blank\">https://wandb.ai/maciejej-uniwersytet-dzki/llm1/runs/0ae7ihsi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logging\n",
    "wandb_log = True\n",
    "wandb_project = 'llm1'\n",
    "wandb_run_name = 'llm1-' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5225f6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that was used to represent a team in an old TV show, The A-Team. A capital a is written \"A\". Use a capital A at the start of a sentence if writing.\n",
      "\n",
      "A is also a musical note, sometimes referred to as \"La\".\n",
      "\n",
      "The letter 'A' was in the Phoenician alphabet's aleph. This symbol came from a simple pictur\n"
     ]
    }
   ],
   "source": [
    "with open(\"wiki.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[10000:10300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f68aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 4096\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "sp = spm.SentencePieceProcessor(model_file=\"wiki_tokenizer.model\")\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da1e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[316, 428, 4052, 4037, 599, 395, 316, 428, 4052, 412, 4055, 428]\n",
      "niebo jest niebieskie\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "zdanie = \"niebo jest niebieskie\"\n",
    "print(encode(zdanie))\n",
    "print(decode(encode(zdanie)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64b7783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoding\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"encoded_data.pt\"):\n",
    "    print(\"Loading encoding\")\n",
    "    data = torch.load(\"encoded_data.pt\")\n",
    "else:\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data, \"encoded_data.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f592a98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 59.21 Million | Training: 53.29 Million | Validation 5.92 Million \n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "splt = int(0.9 * data_size)\n",
    "train_data = data[:splt]\n",
    "val_data = data[splt:]\n",
    "\n",
    "print(f\"Total data: {data_size / 1e6:.2f} Million | Training: {len(train_data) / 1e6:.2f} Million | Validation {len(val_data) / 1e6:.2f} Million \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837e8459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([4031, 4062, 4059, 4056, 4085, 4053,  666, 1906, 2132,  959],\n",
      "       device='cuda:0')\n",
      "tensor([4062, 4059, 4056, 4085, 4053,  666, 1906, 2132,  959,  339],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    indeces = torch.randint(len(data) - context, (batch_size,))\n",
    "    x = torch.stack([data[i: i+context] for i in indeces]) # (batch_size, sequence_length)\n",
    "    y = torch.stack([data[i+1:i+context+1] for i in indeces])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(\"train\")\n",
    "print(x.shape, y.shape)\n",
    "print(x[0][:10])\n",
    "print(y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56f0f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size) # 3096 x 384\n",
    "        self.positions = nn.Embedding(context, embed_size) # 512 x 384\n",
    "        # self.blocks = nn.Sequential(*[Block(n_heads) for _ in range(n_layers)])\n",
    "        self.layer_normalisation = nn.LayerNorm(embed_size)\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS) # 384 x 4096\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input, targets=None):\n",
    "        loss = None\n",
    "        BS, SL = input.shape # BS x SL\n",
    "        emb = self.embeddings(input) # BSS x SL x 384\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # SL x 384\n",
    "        x = emb + pos # BS x SL x 384\n",
    "        # x = self.blocks(x) # BS x SL x 384\n",
    "        x = self.layer_normalisation(x) # BS x SL x Embedding size\n",
    "        logits = self.final_linear(x) # BS x SL x vocab_size (4096)\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, vocabsize = logits.shape\n",
    "            logits = logits.view(BS * SL, vocabsize)\n",
    "            targets = targets.view(BS * SL)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, input, max=500):\n",
    "        for _ in range(max):\n",
    "            input = input[:, -context:] # (1, input length until max of sequence length)\n",
    "            logits, _ = self(input) # (1, input length, vocab_size)\n",
    "            logits = logits[:, -1, :] # pick last logit (1, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (1, vocab_size)\n",
    "            next = torch.multinomial(probs, num_samples=1)\n",
    "            input = torch.cat((input, next), dim=1)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0dc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.multi_attention = Multihead(n_heads, head_size)\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_attention(self.ln1)\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size, 6*embed_size, bias=BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embed_size, embed_size, bias=BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcab949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias=BIAS)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=1)\n",
    "        x = self.combine(x) # (BS, SL, 384)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.keys = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.values = nn.Linear(embed_size, head_size, bias=BIAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c27481f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed: 384 n_heads: 7 head_size: 54\n"
     ]
    }
   ],
   "source": [
    "head_size = embed_size // n_heads\n",
    "print(f\"embed: {embed_size} n_heads: {n_heads} head_size: {head_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02f1d0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([4036, 4053,  347,  305,  999, 4053, 3272, 4043, 4051,   13],\n",
      "       device='cuda:0')\n",
      "tensor([4053,  347,  305,  999, 4053, 3272, 4043, 4051,   13,   13],\n",
      "       device='cuda:0')\n",
      "8.375\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(\"train\")\n",
    "print(x.shape, y.shape)\n",
    "print(x[0][:10])\n",
    "print(y[0][:10])\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "logits, loss = model(x, y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd6284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time recognchester project Jes Julyrightso Fox^ moreoung Polish insp prot internationalheast Court ro Re meellaapt Rockrel English main Lee Flor producer She theseade video night Char esc Fore dayouthmanig Bro Dou Ind Met South bet\u0014 enough evO Aï¿½eld bas Jamesootball claim lawyer Korean resth Polight\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device)\n",
    "    t1 = t1[None, :]\n",
    "    newgen = model.generate(t1, max=64)[0].tolist()\n",
    "    result = decode(newgen)\n",
    "    print(f\"{result}\")\n",
    "\n",
    "generate_sample(\"Once upon a time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13762805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
