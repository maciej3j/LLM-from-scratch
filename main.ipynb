{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90629667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import ipdb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import requests, zipfile, io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# thsese improve performance for Ampere architecture\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5187d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_url = \"https://ideami.com/llm_train\"\n",
    "# print(\"Downloading dataset...\")\n",
    "# response = requests.get(files_url)\n",
    "# zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "039f7684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# architecture parameters\n",
    "batch_size = 8\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = True\n",
    "\n",
    "# hyperparameters\n",
    "lr = 3e-4\n",
    "dropout = 0.05\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "\n",
    "# training parameters\n",
    "train_iters = 100000\n",
    "eval_interval = 50\n",
    "eval_iters = 10\n",
    "compile = True\n",
    "checkpoint_dir = 'models/'\n",
    "checkpoint_load_fn = 'latest.pt'\n",
    "load_pretrained = True\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# MODE \n",
    "inference = False\n",
    "\n",
    "# DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31a39a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llm1-2025_12_14_14_54_21</strong> at: <a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm1/runs/oh9gb9fg' target=\"_blank\">https://wandb.ai/maciejej-uniwersytet-dzki/llm1/runs/oh9gb9fg</a><br> View project at: <a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm1' target=\"_blank\">https://wandb.ai/maciejej-uniwersytet-dzki/llm1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251214_145422-oh9gb9fg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/maciej/workspace/LLMUdemy/wandb/run-20251214_152516-j616oy26</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm-from-scratch/runs/j616oy26' target=\"_blank\">llm-test-run</a></strong> to <a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm-from-scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm-from-scratch' target=\"_blank\">https://wandb.ai/maciejej-uniwersytet-dzki/llm-from-scratch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maciejej-uniwersytet-dzki/llm-from-scratch/runs/j616oy26' target=\"_blank\">https://wandb.ai/maciejej-uniwersytet-dzki/llm-from-scratch/runs/j616oy26</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logging\n",
    "wandb_log = True\n",
    "wandb_project = 'llm-from-scratch'\n",
    "# wandb_run_name = 'llm1-' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "wandb_run_name = 'llm-test-run'\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5225f6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that was used to represent a team in an old TV show, The A-Team. A capital a is written \"A\". Use a capital A at the start of a sentence if writing.\n",
      "\n",
      "A is also a musical note, sometimes referred to as \"La\".\n",
      "\n",
      "The letter 'A' was in the Phoenician alphabet's aleph. This symbol came from a simple pictur\n"
     ]
    }
   ],
   "source": [
    "with open(\"wiki.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[10000:10300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53f68aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 4096\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "sp = spm.SentencePieceProcessor(model_file=\"wiki_tokenizer.model\")\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5da1e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[316, 428, 4052, 4037, 599, 395, 316, 428, 4052, 412, 4055, 428]\n",
      "niebo jest niebieskie\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "zdanie = \"niebo jest niebieskie\"\n",
    "print(encode(zdanie))\n",
    "print(decode(encode(zdanie)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64b7783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoding\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"encoded_data.pt\"):\n",
    "    print(\"Loading encoding\")\n",
    "    data = torch.load(\"encoded_data.pt\")\n",
    "else:\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data, \"encoded_data.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f592a98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 59.21 Million | Training: 53.29 Million | Validation 5.92 Million \n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "splt = int(0.9 * data_size)\n",
    "train_data = data[:splt]\n",
    "val_data = data[splt:]\n",
    "\n",
    "print(f\"Total data: {data_size / 1e6:.2f} Million | Training: {len(train_data) / 1e6:.2f} Million | Validation {len(val_data) / 1e6:.2f} Million \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "837e8459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([  13,  764, 1674,  879,  266, 1836,  299,  264,  926, 1836],\n",
      "       device='cuda:0')\n",
      "tensor([ 764, 1674,  879,  266, 1836,  299,  264,  926, 1836,  280],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    indeces = torch.randint(len(data) - context, (batch_size,))\n",
    "    x = torch.stack([data[i: i+context] for i in indeces]) # (batch_size, sequence_length)\n",
    "    y = torch.stack([data[i+1:i+context+1] for i in indeces])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(\"train\")\n",
    "print(x.shape, y.shape)\n",
    "print(x[0][:10])\n",
    "print(y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56f0f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size) # 3096 x 384\n",
    "        self.positions = nn.Embedding(context, embed_size) # 512 x 384\n",
    "        # self.blocks = nn.Sequential(*[Block(n_heads) for _ in range(n_layers)])\n",
    "        self.layer_normalisation = nn.LayerNorm(embed_size)\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS) # 384 x 4096\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input, targets=None):\n",
    "        loss = None\n",
    "        BS, SL = input.shape # BS x SL\n",
    "        emb = self.embeddings(input) # BSS x SL x 384\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # SL x 384\n",
    "        x = emb + pos # BS x SL x 384\n",
    "        # x = self.blocks(x) # BS x SL x 384\n",
    "        x = self.layer_normalisation(x) # BS x SL x Embedding size\n",
    "        logits = self.final_linear(x) # BS x SL x vocab_size (4096)\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, vocabsize = logits.shape\n",
    "            logits = logits.view(BS * SL, vocabsize)\n",
    "            targets = targets.view(BS * SL)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, input, max=500):\n",
    "        for _ in range(max):\n",
    "            input = input[:, -context:] # (1, input length until max of sequence length)\n",
    "            logits, _ = self(input) # (1, input length, vocab_size)\n",
    "            logits = logits[:, -1, :] # pick last logit (1, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (1, vocab_size)\n",
    "            next = torch.multinomial(probs, num_samples=1)\n",
    "            input = torch.cat((input, next), dim=1)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f0dc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.multi_attention = Multihead(n_heads, head_size)\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_attention(self.ln1)\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d50e12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size, 6*embed_size, bias=BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embed_size, embed_size, bias=BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fbcab949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias=BIAS)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=1)\n",
    "        x = self.combine(x) # (BS, SL, 384)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ab0d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.keys = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.values = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(context, context)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        BS, SL, VS = x.shape\n",
    "        q = self.queries(x) # BS x SL x 54\n",
    "        k = self.keys(x) # BS x SL x 54\n",
    "        v = self.values(x) # BS x SL x 54\n",
    "\n",
    "        # attention scores\n",
    "        attn_w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # BS, SL, SL\n",
    "        attn_w = attn_w.masked_fill(self.tril[:SL, :SL]==0, float('-inf'))\n",
    "        attn_w = F.softmax(attn_w, dim=-1) # BS x SL x SL\n",
    "\n",
    "        x = attn_w @ v # BS x SL x 54\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51d37449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.375\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(\"train\")\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "logits, loss = model(x, y)\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeihnuptside pres heartake bookanioachesokeouri owneball based Loveecut� frog Atlantive got album l federal Sm Februarychestra Laounds crick supportamily years here canton eng beginningfessachusetts�minarch released industryivid tournament these live recordedc Siless womenTealand pointhic Walesrod Mont�ance\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device)\n",
    "    t1 = t1[None, :]\n",
    "    newgen = model.generate(t1, max=64)[0].tolist()\n",
    "    result = decode(newgen)\n",
    "    print(f\"{result}\")\n",
    "\n",
    "# generate_sample(\"Once upon a time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13762805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch :: Compiling model\n",
      "3.3472  Million parameters\n"
     ]
    }
   ],
   "source": [
    "# TRAINING SETUP\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "if compile:\n",
    "    print(\"Torch :: Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \" Million parameters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c52a52d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: NVIDIA GeForce RTX 2060 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 8.375, 'eval': 8.375}\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in [\"train\", \"eval\"]:\n",
    "        l = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            l[i] = loss\n",
    "        out[split] = l.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "l = calculate_loss(\n",
    ")\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "abfcea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the optimizer\n",
    "parameter_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad}\n",
    "weight_decay_p = [p for n, p in parameter_dict.items() if p.dim() >= 2]\n",
    "no_weight_decay_p = [p for n, p in parameter_dict.items() if p.dim() < 2]\n",
    "optimizer_groups = [\n",
    "    {\n",
    "        'params': weight_decay_p, \n",
    "        'weight_decay': weight_decay\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'params': no_weight_decay_p, 'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=lr, betas=(0.9, 0.99))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d169be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading checkpoints\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    print(\"LLM - Loading model\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    iteration = checkpoint['iteration']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Loaded iter {iteration} with loss {loss}\")\n",
    "    return iteration, loss\n",
    "\n",
    "if os.path.exists(f\"{checkpoint_dir}/{checkpoint_load_fn}\") and load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)\n",
    "    best_val_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1821f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "if inference is True:\n",
    "    model.eval()\n",
    "    while True:\n",
    "        qs = input(\"Enter text (q to quit):\\n\")\n",
    "        if qs==\"\":\n",
    "            continue\n",
    "        if qs == \"q\":\n",
    "            break\n",
    "        generate_sample(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "023dc3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: train loss: 8.375 | val loss: 8.375\n",
      "Once upon a time conduct Tor Britainundred To Associationisc places). seen en designed inter Pop couriv Gl Budd involency lawyer stay because taking y party neigh universityulalishedestival\u0016 ordercol whichalthreld countries weremaix couldove Europeanuctiter childrenufros and Arab chang Jackagob movie dataumentsuments Ge Franceof god\n",
      "[CHECKPOINT]: Saving with loss:  8.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/100000 [00:32<15:54:05,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50: train loss: 6.962500095367432 | val loss: 6.915625095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/100000 [00:37<53:27:32,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeemb returnous early et Maxokurp him Song i collegeimaovie eas year data baseract eng dem react the Ju time energyways House Philardsower information Class clatedation v frelf killedence modernthingated snlandsperadu Rich sett\u000e fam and6 Tur Mr Gu ItWation Earthized mediaaster\n",
      "[CHECKPOINT]: Saving with loss:  6.915625095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/100000 [00:58<12:05:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100: train loss: 5.884375095367432 | val loss: 5.787499904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 101/100000 [01:00<31:20:25,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, and. It is governor thous designa kD lineatch-'se) all treidd An times that thearck, 3 ( is does to \"P Leide Johnson caused December 19) is a Hockey aky It isinn rot is roform, the sameors6\n",
      "[CHECKPOINT]: Saving with loss:  5.787499904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 138/100000 [01:15<15:15:08,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted. Cleaning up...\n",
      "GPU memory released\n",
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7fd7d112da90>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fd84183f770, execution_count=71 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fd8407039d0, raw_cell=\"# training loop\n",
      "\n",
      "try:\n",
      "    for i in tqdm(range(star..\" transformed_cell=\"# training loop\n",
      "\n",
      "try:\n",
      "    for i in tqdm(range(star..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/maciej/workspace/LLMUdemy/main.ipynb#X31sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/wandb/sdk/wandb_init.py:604\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/wandb/sdk/interface/interface.py:811\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    810\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/asyncio/streams.py:386\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(start_iteration, train_iters)):\n",
    "        xb, yb = get_batch(\"train\")\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        if (i % eval_interval==0 or i == train_iters - 1):\n",
    "            l=calculate_loss()\n",
    "            print(f\"\\n{i}: train loss: {l['train']} | val loss: {l['eval']}\")\n",
    "            generate_sample(\"Once upon a time\")\n",
    "\n",
    "            if l['eval'] < best_val_loss:\n",
    "                best_val_loss = l['eval']\n",
    "                print('[CHECKPOINT]: Saving with loss: ', best_val_loss)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_val_loss,\n",
    "                    'iteration': i,\n",
    "\n",
    "                }, checkpoint_dir + checkpoint_load_fn)\n",
    "\n",
    "                if wandb_log:\n",
    "                    wandb.log({\n",
    "                        \"loss/train\": l[\"train\"],\n",
    "                        \"loss/val\": l[\"eval\"],\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "\n",
    "                    }, step = i)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory released\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d1172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
